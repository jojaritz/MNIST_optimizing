{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPMJAjRQXN83QkW9v/mDh43",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojaritz/MNIST_optimizing/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FDgagJ73Awgv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE =64 #how many images get processed at the same time (a GPU)\n",
        "LEARING_RATE = 0.1\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "YPcS8cJZDspN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,)) #centers values so that values range from -1 to 1\n",
        "])"
      ],
      "metadata": {
        "id": "ZW3WVBCbD3lh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "kmgWA1iZFN5x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "#nn.Conv2d it takes each of the values in the 3x3 , then multiplies it by a weight and adds them, then adds a bias (the function uses a bell curve to choose initial random values)\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels = 32, kernel_size=3, stride=1)#kernal size is how many pixels (3x3) for its shape; stride is how much the frame moves each time\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1) #the second one is building on the data that it first got ^ and gets a bigger picture of what happened\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=64*12*12, out_features=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "\n",
        "    x= self.conv2(x)\n",
        "\n",
        "    x= torch.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "\n",
        "    x = torch.flatten(x,1)\n",
        "\n",
        "    x=self.fc1(x) # this is basically a version of conv but instead it looks at all of the pixel data and makes the wieghts into 10 outputs (a kernal of all pixels *26)\n",
        "\n",
        "    return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARING_RATE)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    loss = criterion(outputs,labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if(i+1) % 100 ==0:\n",
        "      print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
        "      running_loss = 0.0\n",
        "  scheduler.step()\n",
        "  current_lr = scheduler.get_last_lr()[0]\n",
        "  print(f\"End of Epoch {epoch+1}. New Learning Rate: {current_lr:.5f}\")\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab5YRia1FlXp",
        "outputId": "d922312a-bac3-4ac0-f60b-4e79ff80be82"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda...\n",
            "Epoch [1/10], Step [938], Loss: 0.6059\n",
            "Epoch [1/10], Step [938], Loss: 0.1753\n",
            "Epoch [1/10], Step [938], Loss: 0.1446\n",
            "Epoch [1/10], Step [938], Loss: 0.1071\n",
            "Epoch [1/10], Step [938], Loss: 0.1031\n",
            "Epoch [1/10], Step [938], Loss: 0.0907\n",
            "Epoch [1/10], Step [938], Loss: 0.0848\n",
            "Epoch [1/10], Step [938], Loss: 0.0768\n",
            "Epoch [1/10], Step [938], Loss: 0.0750\n",
            "End of Epoch 1. New Learning Rate: 0.07000\n",
            "Epoch [2/10], Step [938], Loss: 0.0596\n",
            "Epoch [2/10], Step [938], Loss: 0.0479\n",
            "Epoch [2/10], Step [938], Loss: 0.0569\n",
            "Epoch [2/10], Step [938], Loss: 0.0508\n",
            "Epoch [2/10], Step [938], Loss: 0.0561\n",
            "Epoch [2/10], Step [938], Loss: 0.0529\n",
            "Epoch [2/10], Step [938], Loss: 0.0494\n",
            "Epoch [2/10], Step [938], Loss: 0.0506\n",
            "Epoch [2/10], Step [938], Loss: 0.0405\n",
            "End of Epoch 2. New Learning Rate: 0.04900\n",
            "Epoch [3/10], Step [938], Loss: 0.0385\n",
            "Epoch [3/10], Step [938], Loss: 0.0381\n",
            "Epoch [3/10], Step [938], Loss: 0.0375\n",
            "Epoch [3/10], Step [938], Loss: 0.0317\n",
            "Epoch [3/10], Step [938], Loss: 0.0453\n",
            "Epoch [3/10], Step [938], Loss: 0.0410\n",
            "Epoch [3/10], Step [938], Loss: 0.0438\n",
            "Epoch [3/10], Step [938], Loss: 0.0391\n",
            "Epoch [3/10], Step [938], Loss: 0.0347\n",
            "End of Epoch 3. New Learning Rate: 0.03430\n",
            "Epoch [4/10], Step [938], Loss: 0.0240\n",
            "Epoch [4/10], Step [938], Loss: 0.0362\n",
            "Epoch [4/10], Step [938], Loss: 0.0279\n",
            "Epoch [4/10], Step [938], Loss: 0.0281\n",
            "Epoch [4/10], Step [938], Loss: 0.0331\n",
            "Epoch [4/10], Step [938], Loss: 0.0333\n",
            "Epoch [4/10], Step [938], Loss: 0.0309\n",
            "Epoch [4/10], Step [938], Loss: 0.0339\n",
            "Epoch [4/10], Step [938], Loss: 0.0344\n",
            "End of Epoch 4. New Learning Rate: 0.02401\n",
            "Epoch [5/10], Step [938], Loss: 0.0252\n",
            "Epoch [5/10], Step [938], Loss: 0.0247\n",
            "Epoch [5/10], Step [938], Loss: 0.0247\n",
            "Epoch [5/10], Step [938], Loss: 0.0277\n",
            "Epoch [5/10], Step [938], Loss: 0.0334\n",
            "Epoch [5/10], Step [938], Loss: 0.0246\n",
            "Epoch [5/10], Step [938], Loss: 0.0255\n",
            "Epoch [5/10], Step [938], Loss: 0.0258\n",
            "Epoch [5/10], Step [938], Loss: 0.0275\n",
            "End of Epoch 5. New Learning Rate: 0.01681\n",
            "Epoch [6/10], Step [938], Loss: 0.0249\n",
            "Epoch [6/10], Step [938], Loss: 0.0207\n",
            "Epoch [6/10], Step [938], Loss: 0.0203\n",
            "Epoch [6/10], Step [938], Loss: 0.0278\n",
            "Epoch [6/10], Step [938], Loss: 0.0279\n",
            "Epoch [6/10], Step [938], Loss: 0.0188\n",
            "Epoch [6/10], Step [938], Loss: 0.0205\n",
            "Epoch [6/10], Step [938], Loss: 0.0256\n",
            "Epoch [6/10], Step [938], Loss: 0.0244\n",
            "End of Epoch 6. New Learning Rate: 0.01176\n",
            "Epoch [7/10], Step [938], Loss: 0.0176\n",
            "Epoch [7/10], Step [938], Loss: 0.0192\n",
            "Epoch [7/10], Step [938], Loss: 0.0226\n",
            "Epoch [7/10], Step [938], Loss: 0.0243\n",
            "Epoch [7/10], Step [938], Loss: 0.0233\n",
            "Epoch [7/10], Step [938], Loss: 0.0229\n",
            "Epoch [7/10], Step [938], Loss: 0.0215\n",
            "Epoch [7/10], Step [938], Loss: 0.0238\n",
            "Epoch [7/10], Step [938], Loss: 0.0208\n",
            "End of Epoch 7. New Learning Rate: 0.00824\n",
            "Epoch [8/10], Step [938], Loss: 0.0201\n",
            "Epoch [8/10], Step [938], Loss: 0.0213\n",
            "Epoch [8/10], Step [938], Loss: 0.0227\n",
            "Epoch [8/10], Step [938], Loss: 0.0226\n",
            "Epoch [8/10], Step [938], Loss: 0.0200\n",
            "Epoch [8/10], Step [938], Loss: 0.0247\n",
            "Epoch [8/10], Step [938], Loss: 0.0176\n",
            "Epoch [8/10], Step [938], Loss: 0.0178\n",
            "Epoch [8/10], Step [938], Loss: 0.0173\n",
            "End of Epoch 8. New Learning Rate: 0.00576\n",
            "Epoch [9/10], Step [938], Loss: 0.0217\n",
            "Epoch [9/10], Step [938], Loss: 0.0188\n",
            "Epoch [9/10], Step [938], Loss: 0.0185\n",
            "Epoch [9/10], Step [938], Loss: 0.0175\n",
            "Epoch [9/10], Step [938], Loss: 0.0194\n",
            "Epoch [9/10], Step [938], Loss: 0.0214\n",
            "Epoch [9/10], Step [938], Loss: 0.0215\n",
            "Epoch [9/10], Step [938], Loss: 0.0216\n",
            "Epoch [9/10], Step [938], Loss: 0.0163\n",
            "End of Epoch 9. New Learning Rate: 0.00404\n",
            "Epoch [10/10], Step [938], Loss: 0.0174\n",
            "Epoch [10/10], Step [938], Loss: 0.0222\n",
            "Epoch [10/10], Step [938], Loss: 0.0132\n",
            "Epoch [10/10], Step [938], Loss: 0.0182\n",
            "Epoch [10/10], Step [938], Loss: 0.0201\n",
            "Epoch [10/10], Step [938], Loss: 0.0259\n",
            "Epoch [10/10], Step [938], Loss: 0.0192\n",
            "Epoch [10/10], Step [938], Loss: 0.0180\n",
            "Epoch [10/10], Step [938], Loss: 0.0193\n",
            "End of Epoch 10. New Learning Rate: 0.00282\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. THE TESTBENCH (Quality Control) ---\n",
        "# Note: We use train=False to get the 10,000 images the model has NEVER seen.\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nStarting Testing...\")\n",
        "\n",
        "# 1. Lock the model (Batch Norm/Dropout behave differently in Test mode)\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# 2. Turn off the Engine (No Gradients = No Learning, just Predicting)\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Calculate outputs\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Get the predicted class (The index of the highest score)\n",
        "        # _ is the max value (we don't care), predicted is the index (we care)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the 10,000 test images: {accuracy:.2f}%')\n",
        "\n",
        "# Optional: Save the model weights for Dr. Wang to inspect later\n",
        "torch.save(model.state_dict(), 'mnist_cnn_hardware_ready.pth')\n",
        "print(\"Model saved to mnist_cnn_hardware_ready.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDkom0cfniSs",
        "outputId": "2012cf09-1e0b-41b6-8945-7883c4e2715e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Testing...\n",
            "Accuracy of the model on the 10,000 test images: 98.81%\n",
            "Model saved to mnist_cnn_hardware_ready.pth\n"
          ]
        }
      ]
    }
  ]
}